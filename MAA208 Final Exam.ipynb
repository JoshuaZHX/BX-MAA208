{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final exam: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** the exam is composed of three exercises including theoretical and implementation parts. Every exercise can be treated independently.\n",
    "- This document is available from the 18th March at 8 AM and must be uploaded back **BEFORE the 26th at 8 PM** (take your precautions to upload it in time).   \n",
    "- Answering in markdown in the notebook is encouraged, but if you have difficulties with it, you are allowed to answer the theoretical part on a separate document in pdf, png, or jpg format.\n",
    "- In both cases, the clarity (clean writing) and the organization of the work (numbering the questions appropriately) are taken into account during the correction. \n",
    "- You are allowed to use any document (script, slides, other internet sources...).\n",
    "- Before uploading your notebook, **clean it (Kernel/restart and clear output)** to make it  lighter. And **verify that your code is running cell after cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library and functions used in the notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as lin\n",
    "from scipy.linalg import cho_factor, cho_solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Convergence of tridiagonal Jacobi and Gauss-Seidel methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to compare numerically the convergence rates of Jacobi and Gauss-Seidel algorithms in the case of tridiagonal matrices. We focus on one example given by the matrix of the Laplacian \n",
    "\n",
    "$$L = \\left(\\begin{array}{cccccc} \n",
    "2  & -1 & 0  & \\dots & \\dots & 0 \\\\\n",
    "-1 & 2  & -1 & 0 &  & \\vdots \\\\\n",
    "0  & -1 & 2  & -1 & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0  \\\\ \n",
    "\\vdots &  & \\ddots & \\ddots & \\ddots& -1  \\\\ \n",
    "0 & \\dots & \\dots & 0 & -1 & 2\n",
    "\\end{array}\\right) \\in \\mathbb{R}^{N\\times N}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) a) Implement Jacobi algorithm to solve systems of the form $L V = b$. \n",
    "The arguments of the function are given. There are two stopping criteria, one on the maximal number of iterations and one on the error $\\|AV^n-b\\|$.\n",
    "\n",
    "*Hint: you may exploit the considered tridiagonal structure of the matrix $L$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobi_Laplace(N, b, V0, it_max = 10**4, TOL = 10**(-10)):\n",
    "    \"\"\"\n",
    "    Compute the iterations of Jacobi algorithm for the Laplacian matrix\n",
    "    ----------   \n",
    "    parameters:\n",
    "    N      : size of the matrix NxN\n",
    "    b      : RHS of the problem LV = b where L is the Laplacian matrix\n",
    "    V0     : initial vector in the method\n",
    "    it_max : maximum number of iterations\n",
    "    TOL    : use || L V^k - b || < TOL as a stopping criteria \n",
    "    \n",
    "    returns:\n",
    "    V       : the solution at the end of the iterations\n",
    "    tab_err : array of the || L V^k - b ||^2 at every iteration \n",
    "    \"\"\"\n",
    "    \n",
    "    #initialization \n",
    "    V_new      = np.copy(V0) \n",
    "    V          = np.copy(V_new) \n",
    "    tab_err    = np.zeros(it_max + 1) \n",
    "\n",
    "    #convergence loop \n",
    "    k          = 0 \n",
    "    LV         = matmulLV(N, V_new) \n",
    "    err        = lin.norm(LV - b) \n",
    "    tab_err[0] = err \n",
    "    while (k < it_max) and (err > TOL): \n",
    "        k += 1 \n",
    "        V = np.copy(V_new) \n",
    "        \n",
    "        V_new[0]   = (b[0] + V[1]) / 2 \n",
    "        V_new[N-1] = (b[N-1] + V[N-2]) / 2 \n",
    "        for i in range(1, N-1): \n",
    "            V_new[i] = (b[i] + V[i-1] + V[i+1]) / 2 \n",
    "        \n",
    "        LV         = matmulLV(N, V_new) \n",
    "        err        = lin.norm(LV - b) \n",
    "        tab_err[k] = err \n",
    "    \n",
    "    return V, tab_err[:k+1] \n",
    "\n",
    "\n",
    "def matmulLV(N, V_new): \n",
    "    \"\"\"\n",
    "    Compute the matrix multiplication (L, V_new) in order to compute the error at each iteration. \n",
    "    ---------- \n",
    "    Parameters: \n",
    "    N     : size of the matrix. \n",
    "    V_new : the V_new matrix at each iteration. \n",
    "    \n",
    "    returns: \n",
    "    LV : the matrix multiplication L, V_new. \n",
    "    \"\"\"\n",
    "    LV = np.copy(V_new) \n",
    "    \n",
    "    LV[0]   = 2*V_new[0] - V_new[1] \n",
    "    LV[N-1] = - V_new[N-2] + 2*V_new[N-1] \n",
    "    for i in range(1, N-1): \n",
    "        LV[i] = - V_new[i-1] + 2*V_new[i] - V_new[i+1] \n",
    "    \n",
    "    return LV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test it with the given data, $N= 20$, $b= (1,0,\\dots,0)$ and plot the error obtained with this method as a function of the iteration $n$ in logscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "N = 20 \n",
    "b = np.zeros(N); b[0] = 1.\n",
    "\n",
    "#solving the system\n",
    "sol_J, err_J = Jacobi_Laplace(N, b, b, it_max = 10**4, TOL = 10**(-10))\n",
    "\n",
    "#plot the errors\n",
    "plt.figure(1)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(err_J)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) a) Implement Gauss-Seidel algorithm to solve the system $L V = b$ where $b = (1, 0, \\dots, 0) \\in\\mathbb{R}^N$. \n",
    "The arguments of the function are given.\n",
    "\n",
    "\n",
    "*Hint: you may exploit again the considered tridiagonal structure of the matrix.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GS_Laplace(N, b, V0, it_max = 10**4, TOL = 10**(-10)):\n",
    "    \"\"\"\n",
    "    Compute the iterations of Gauss-Seidel algorithm for the Laplacian matrix\n",
    "    ----------   \n",
    "    parameters:\n",
    "    N      : size of the matrix NxN\n",
    "    b      : RHS of the problem L V = b\n",
    "    V0     : initial vector in the method\n",
    "    it_max : maximum number of iterations\n",
    "    TOL    : use || L V^k - b || < TOL as a stopping criteria \n",
    "    \n",
    "    returns:\n",
    "    V       : the solution at the end of the iterations\n",
    "    tab_err : array of the || L V^k - b ||^2 at every iteration \n",
    "    \"\"\"\n",
    "    \n",
    "    #initialization\n",
    "    V_new   = np.copy(V0)\n",
    "    V       = np.copy(V_new)\n",
    "    tab_err = np.zeros(it_max+1)\n",
    "\n",
    "    #convergence loop\n",
    "    k          = 0 \n",
    "    LV         = matmulLV(N, V_new) \n",
    "    err        = lin.norm(LV - b) \n",
    "    tab_err[0] = err \n",
    "    while (k < it_max) and (err > TOL): \n",
    "        k += 1 \n",
    "        V = np.copy(V_new) \n",
    "        \n",
    "        V_new[0] = (b[0] + V[1]) / 2 \n",
    "        for i in range(1, N-1): \n",
    "            V_new[i] = (b[i] + V_new[i-1] + V[i+1]) / 2 \n",
    "        V_new[N-1] = (b[N-1] + V_new[N-2]) / 2 \n",
    "        \n",
    "        LV         = matmulLV(N, V_new) \n",
    "        err        = lin.norm(LV - b) \n",
    "        tab_err[k] = err \n",
    "    \n",
    "    return V, tab_err[:k+1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test it with the given data, $N= 20$, $b= (1,0,\\dots,0)$. Compare the behaviour (convergence rate, number of iterations, ...) of Jacobi and Gauss-Seidel algorithms on this test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "N = 20 \n",
    "b = np.zeros(N); b[0] = 1.\n",
    "\n",
    "#solving the system\n",
    "sol_GS, err_GS = GS_Laplace(N, b, b, it_max = 10**4, TOL = 10**(-10))\n",
    "\n",
    "#plot the errors\n",
    "plt.figure(2)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(err_J)\n",
    "plt.plot(err_GS)\n",
    "plt.legend([\"Jacobi\",\"Gauss-Seidel\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "- We see that the convergence rate of the Gauss-Seidel algorithm is faster than the convergence rate of the Jacobi algorithm, because the slope of the yellow curve is more steep than the slop of the blue curve. \n",
    "\n",
    "- We see that the number of iterations required of the Gauss-Seidel algorithm is less than the number of iterations required of the Jacobi algorithm, which is clearly indicated by the graph above on the x-axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compute numerically the spectral radii of Jacobi and Gauss-Seidel iteration matrices using the convergence rates of those methods on the last test case. Compare them for different values of N. \n",
    "\n",
    "(Bonus): For the considered matrix, the two spectral radii are related to each others (one is a function of the other). Find numerically this relation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_J  = err_J[-1] / err_J[-2] \n",
    "print(f\"rho Jacobi       : {rho_J}\"   )\n",
    "\n",
    "rho_GS  = err_GS[-1] / err_GS[-2] \n",
    "print(f\"rho Gauss-Seidel : {rho_GS}\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "For $N = 20$, the spectral radii are \n",
    "- Jacobi : 0.9888311798960567\n",
    "- Gauss-Seidel : 0.9777864458910915 \n",
    "\n",
    "For $N = 100$, the spectral radii are \n",
    "- Jacobi : 0.9995162822925521\n",
    "- Gauss-Seidel : 0.9990327985678236\n",
    "\n",
    "Therefore, we see that as $N$ increases, the spectra radius of both Jacobi and Gauss-Seidel algorithm gradually converges to 1. Moreover, the spectral radii of Jacobi is greater than the spectra radii of Gauss-Seidel, implying that for our chosen $N$, the Gauss-Seidel algorithm converges faster than the Jacobi algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Complexity of Strassen's multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historical context:** In the late 60's, the operations performed by computers were slower, especially, multiplications in floating point arithmetic was around three times slower than additions. To compensate that, Winograd in 1967, then Strassen in 1969 studied factorizations of matrix products in order to dicrease the number of operations that these required (especially the number of multiplications). This factorization also lead to direct methods for the computation of inverse matrices that required less operations (for large matrices $N\\rightarrow \\infty$) than the classical Gaussian elimination.\n",
    "\n",
    "In this exercise, we compare at a theoretical level the number of operations (additions and multiplications together) required when using the basic formula of matrix-matrix product and when using Strassen formulae. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Basic computation of the product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Consider two vectors $V\\in\\mathbb{R}^N$ and $W\\in\\mathbb{R}^N$. How many operations are required to compute their dot product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Their dot product is computed as \n",
    "\n",
    "$$ \n",
    "V \\cdot W = \n",
    "\\begin{pmatrix} \n",
    "    V_1 \\\\ V_2 \\\\ \\vdots \\\\ V_N \n",
    "\\end{pmatrix} \\cdot \n",
    "\\begin{pmatrix} \n",
    "    W_1 \\\\ W_2 \\\\ \\vdots \\\\ W_N \n",
    "\\end{pmatrix} = \n",
    "V_1 W_1 + V_2 W_2 + \\cdots + V_N W_N \n",
    "$$ \n",
    "\n",
    "Hence, there are in total $N$ multiplications and $N - 1$ additions. Therefore, the total operations required is $2N - 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Deduce the number of operations required to compute the matrix product $AB$ of two matrices of $\\mathbb{R}^{N\\times N}$ by computing each of its component as the dot product of two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Each component of the matrix $AB$ requires $2N - 1$ operations as the dot product of two vectors. Since the matrix $AB$ has in total $N^2$ components, we can deduce that the total operations required is \n",
    "\n",
    "$$ \n",
    "(2N - 1) \\times N^2 = 2N^3 - N^2 \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Compare the number of operations required to do the product of two $N\\times N$ matrices with the one of two $2N\\times 2N$ matrices when $N$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "The number of operations to compute the product of two $N \\times N$ matrices is $M_{N} = 2N^3 - N^2$. \n",
    "\n",
    "The number of operations to compute the product of two $2N \\times 2N$ matrices is $M_{2N} = 2 (2N)^3 - (2N)^2 = 16N^3 - 4N^2$. \n",
    "\n",
    "Hence, we obtain that $M_{2N} = 8 M_{N} + 4 N^2$. \n",
    "\n",
    "Moreover, when $N$ is large, we know that the $N^3$ term will dominate and $N^2$ term is negligible. As a result, we can approximate it as $M_{2N} \\approx 8 M_{N}$. Therefore, computing the product of two $2N \\times 2N$ matrices requires 8 times more operations than computing the product of two $N \\times N$ matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Strassen's factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strassen's formulae is provided but the  construction is omited here. Only the study of its complexity is focussed on. We will compute the number of operations required through this factorization and compare it with the one obtain in the previous section.\n",
    "\n",
    "The matrices $A$ and $B$ are of size $\\mathbb{R}^N$ where $N = 2^p$ is some power of 2. Decompose all matrices into block form of equal size \n",
    "\n",
    "$$ A = \\left(\\begin{array}{c|c} A_{1,1} & A_{1,2} \\\\ \\hline A_{2,1} & A_{2,2}\\end{array}\\right), \\quad B = \\left(\\begin{array}{c|c} B_{1,1} & B_{1,2} \\\\ \\hline B_{2,1} & B_{2,2}\\end{array}\\right), \\quad C = AB = \\left(\\begin{array}{c|c} C_{1,1} & C_{1,2} \\\\ \\hline C_{2,1} & C_{2,2}\\end{array}\\right),$$\n",
    "\n",
    "where $A_{i,j}$, $B_{i,j}$ and $C_{i,j}$ are submatrices of size $\\frac{N}{2}\\times \\frac{N}{2}.$\n",
    "\n",
    "Then Strassen provided the following 7 matrices \n",
    "\n",
    "\\begin{align*}\n",
    "    P_1 &= (A_{1,1}+A_{2,2})(B_{1,1}+B_{2,2}) \\\\\n",
    "    P_2 &= (A_{2,1}+A_{2,2,})B_{1,1} \\\\\n",
    "    P_3 &= A_{1,1} (B_{1,2}-B_{2,2}) \\\\\n",
    "    P_4 &= A_{2,2}(B_{2,1}-B_{1,1}) \\\\\n",
    "    P_5 &= (A_{1,1}+A_{1,2})B_{2,2} \\\\\n",
    "    P_6 &= (A_{2,1}-A_{1,1})(B_{1,1}+B_{1,2}) \\\\\n",
    "    P_7 &= (A_{1,2}-A_{2,2})(B_{2,1}-B_{2,2})\n",
    "\\end{align*}\n",
    "\n",
    "such that the matrix product $C= AB$ yields \n",
    "\n",
    "\\begin{align*}\n",
    "    C_{1,1} &= P_1 + P_4 - P_5 + P_7 &= A_{1,1}B_{1,1} + A_{1,2} B_{2,1} \\\\\n",
    "    C_{1,2} &= P_3 + P_5             &= A_{1,1}B_{1,2} + A_{1,2} B_{2,2} \\\\\n",
    "    C_{2,1} &= P_2 + P_4             &= A_{2,1}B_{1,1} + A_{2,2} B_{2,1} \\\\\n",
    "    C_{2,2} &= P_1 + P_3 - P_2 + P_6 &= A_{2,1}B_{1,2} + A_{2,2} B_{2,2}.\n",
    "\\end{align*}\n",
    "\n",
    "We aim to compute the number of operations required to obtain $C$ through the knowledge of the $P_i$'s.\n",
    "\n",
    "For this purpose, write $S_{N}$ the number of operations required to compute the sum of two matrices of size $N$ and $M_N$ the number of operations required to compute the multiplication of two matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Compute $S_N$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "To sum two matrices, we simply need to add each component of the two matrices together. Since there are in total $N^2$ components for each matrix, and each one of them requires only one addition, we can conclude that the total number of operations required is $S_N = N^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Assuming that $A$ and $B$ are of size ${N\\times N}$, how many operations are required to compute $P_1$ (or equivalently $P_6$ or $P_7$) ? Express it as a function of $S_{N/2}$ and $M_{N/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "For computing $P_1$ (or equivalently $P_6$, $P_7$), we need to sum two matrices of size $N/2$ twice and multiply two matrices of size $N/2$ once. Thus, the total number of operations required is $2 S_{N/2} + M_{N/2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Assuming that $A$ and $B$ are of size ${N\\times N}$, how many operations are required to compute $P_2$ (or equivalently $P_3$, $P_4$ or $P_5$) ? Express it as a function of $S_{N/2}$ and $M_{N/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "For computing $P_2$ (or equivalently $P_3$, $P_4$, $P_5$), we need to sum two matrices of size $N/2$ once and multiply two matrices of size $N/2$ once. Thus, the total number of operations required is $S_{N/2} + M_{N/2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Knowing $P_i$ for $i=1,...,7$, how many operations are required to obtain $C = AB$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "By definition, we know that \n",
    "- $C_{1,1}$ is the sum of 4 matrices of size $N/2$. Hence, the number of operations required is $3 S_{N/2}$. \n",
    "- $C_{1,2}$ is the sum of 2 matrices of size $N/2$. Hence, the number of operations required is $S_{N/2}$. \n",
    "- $C_{2,1}$ is the sum of 2 matrices of size $N/2$. Hence, the number of operations required is $S_{N/2}$. \n",
    "- $C_{2,2}$ is the sum of 4 matrices of size $N/2$. Hence, the number of operations required is $3 S_{N/2}$. \n",
    "\n",
    "Therefore, the total number of operations required to compute $C$ is $8 S_{N/2}$ given $P_i$ for $i = \\{ 1, \\dots, 7 \\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Deduce $M_{N}$ as a function of $M_{N/2}$ and $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "The total number of operations required to compute $C$ is the sum of the number of operations required to compute $P_i$ for $i \\in \\{ 1, \\dots, 7 \\}$ and the number of operations required to compute $C$ given $P_i$. Therefore, we obtain that \n",
    "\n",
    "$$ \n",
    "M_{N} = 3 \\left( 2 S_{N/2} + M_{N/2} \\right) + 4 \\left( S_{N/2} + M_{N/2} \\right) + 8 S_{N/2} = 18 S_{N/2} + 7 M_{N/2} \n",
    "$$ \n",
    "\n",
    "Since $S_{N/2} = \\left( \\frac{N}{2} \\right)^2 = \\frac{N^2}{4}$, we can conclude that \n",
    "\n",
    "$$ \n",
    "M_{N} = 18 \\left( \\frac{N^2}{4} \\right) + 7 M_{N/2} = 4.5 N^2 + 7 M_{N/2} \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Compare this raise with the one found in **Part 1-3)** when $N$ is large. Does Strassen's method constitute an improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Given $M_{N}$ requiring $\\propto N^3$ number of operations, we can deduce that \n",
    "\n",
    "- For the Strassen's method, $M_{2N} = 7 M_{N} + 18 N^2 \\approx 7 M_{N}$ when $N$ is large. \n",
    "\n",
    "- For the basic computation method, $M_{2N} = 8 M_{N} + 4 N^2 \\approx 8 M_{N}$ when $N$ is large. \n",
    "\n",
    "Therefore, we can see that Strassen's method requires less operations and thus it constitutes an improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Incomplete Cholesky factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we aim to accelerate the convergence of an iterative method by exploiting a direct method. We illustrate this method again on a matrix closely related to Laplacian, i.e. \n",
    "\n",
    "$$ A_{i,j} = 2 \\left(1+\\frac{1}{N}\\right) \\delta_{i,j} - \\delta_{i+1,j} - \\delta_{i-1,j} - \\delta_{i,1}\\delta_{j,N} - \\delta_{i,N}\\delta_{j,1}. \\qquad{} (3.1)$$\n",
    "\n",
    "The construction of this matrix is implemented in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Construct_Matrix(N):\n",
    "    #Construct the desired matrix of size NxN\n",
    "    A  = 2.*(1+1/N)*np.eye(N)-np.diag(np.ones(N-1),1)-np.diag(np.ones(N-1),-1)\n",
    "    A[0,-1] -= 1.\n",
    "    A[-1,0] -= 1.  \n",
    "    return A\n",
    "\n",
    "print(\"Matrix(10): \\n\",Construct_Matrix(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Foreword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Consider the problem $AV = b$.\n",
    "Prove that Jacobi algorithm converges toward $V^\\infty = A^{-1}b$ for all initialization $V^0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "First, we prove that $A$ is diagonally strictly dominant. By definition, we know that \n",
    "\n",
    "$$\n",
    "A = \\left(\\begin{array}{cccccc} \n",
    "2 + \\frac{2}{N} & -1 & 0  & \\dots & \\dots & -1 \\\\\n",
    "-1 & 2 + \\frac{2}{N} & -1 & 0 &  & \\vdots \\\\\n",
    "0  & -1 & 2 + \\frac{2}{N} & -1 & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0  \\\\ \n",
    "\\vdots &  & \\ddots & \\ddots & \\ddots& -1  \\\\ \n",
    "-1 & \\dots & \\dots & 0 & -1 & 2 + \\frac{2}{N} \n",
    "\\end{array}\\right) \n",
    "$$\n",
    "\n",
    "Fix a row $i \\in \\{ 1, \\dots, N \\}$. Then, \n",
    "- The diagonal component in row $i$ is $A_{i,i} = 2 \\left( 1 + \\frac{1}{N} \\right) = 2 + \\frac{2}{N}$. \n",
    "- The sum of the remaining components in row $i$ is $\\sum_{j \\neq i} \\left| A_{i,j} \\right| = 2$. \n",
    "\n",
    "Since $\\left| A_{i,i} \\right| > \\sum_{j \\neq i} \\left| A_{i,j} \\right|$ for every row in the matrix $A$, we deduce that $A$ is diagonally strictly dominant. Therefore, the Jacobi algorithm always converges for all initialization $V^0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Jacobi algorithm is implemented (see below) and tested in the next cells. Study this implementation and, based on it, compute numerically the convergence rate of this method. What would be the ideal convergence rate ? And the worst possible such that the method remains convergent ? Is the present convergence fast or slow ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobi(A, b, V0, it_max=1000, TOL=10**(-10)):\n",
    "    \"\"\"\n",
    "    Compute the iterations of Jacobi algorithm\n",
    "    ----------   \n",
    "    parameters:\n",
    "    A      : Matrix of size NxN of the problem A V = b\n",
    "    b      : RHS of the problem A V = b\n",
    "    V0     : initial vector in the method\n",
    "    it_max : maximum number of iterations\n",
    "    TOL    : use || A V^k - b || < TOL as a stopping criteria \n",
    "    \n",
    "    returns:\n",
    "    V       : the solution at the end of the iterations\n",
    "    tab_err : array of the || A V^k - b ||^2 at every iteration\n",
    "    \"\"\"\n",
    "   \n",
    "    # initializations\n",
    "    V_new      = np.copy(V0)\n",
    "    tab_err    = np.zeros(it_max)\n",
    "    tab_err[0] = lin.norm(np.matmul(A,V_new) - b)\n",
    "    \n",
    "    # construction of the matrices\n",
    "    D = np.diag(A)\n",
    "    R = np.diag(D) - A\n",
    "    \n",
    "    # Convergence loop\n",
    "    for k in range(it_max-1):\n",
    "        V = np.copy(V_new)\n",
    "        \n",
    "        if(tab_err[k] < TOL): break\n",
    "        \n",
    "        V_new = (b + np.dot(R, V)) / D\n",
    "        \n",
    "        tab_err[k+1] = lin.norm(np.matmul(A,V_new) - b)\n",
    "        \n",
    "    return V, tab_err[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "N  = 100\n",
    "A  = Construct_Matrix(N)\n",
    "b  = np.ones(N)\n",
    "V0 = np.zeros(N) \n",
    "\n",
    "# solving the linear system\n",
    "U_Jacobi, err_Jacobi = Jacobi(A, b, V0)\n",
    "\n",
    "#plotting the error \n",
    "plt.figure(3)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(err_Jacobi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the convergence rate\n",
    "rho_Jacobi = err_Jacobi[-1] / err_Jacobi[-2] \n",
    "print(f\"rho_Jacobi {rho_Jacobi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "The ideal convergence rate is when the spectral radius is close to $0$. \n",
    "\n",
    "The worst convergence rate is when the spectral radius is close to $1$. \n",
    "\n",
    "Given the spectral radius, we can deduce the convergence rate, which is similar to the sequence $\\left( \\rho (A)^k \\right)_k$. \n",
    "\n",
    "Since the spectral radius of the Jacobi algorithm is $0.9900990099831737$ as computed above for $N = 100$, we can conclude that it converges slowly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we aim to accelerate the convergence of Jacobi algorithm using an incomplete Cholesky factorization.\n",
    "\n",
    "3) The principle of the incomplete Cholesky factorization is to write a symmetric positive definite matrix $A$ under the form \n",
    "$$A = L L^T - R$$\n",
    "where $L$ is lower triangular (and ideally $L$ is sparse, i.e. it has many zero entries and its computation is expected to require less operations). \n",
    "\n",
    "Consider the matrix $A$ defined in (3.1) and the matrix $R$ given by $$R_{i,j} = \\delta_{i,1}\\delta_{j,N} + \\delta_{i,N}\\delta_{j,1}.$$\n",
    "Show that $A$ possesses an incomplete Cholesky factorisation $A = LL^T - R$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "By definition, we know that \n",
    "\n",
    "$$\n",
    "A + R = \\left(\\begin{array}{cccccc} \n",
    "2 + \\frac{2}{N} & -1 & 0  & \\dots & \\dots & 0 \\\\\n",
    "-1 & 2 + \\frac{2}{N} & -1 & 0 &  & \\vdots \\\\\n",
    "0  & -1 & 2 + \\frac{2}{N} & -1 & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0  \\\\ \n",
    "\\vdots &  & \\ddots & \\ddots & \\ddots& -1  \\\\ \n",
    "0 & \\dots & \\dots & 0 & -1 & 2 + \\frac{2}{N} \n",
    "\\end{array}\\right) \n",
    "$$\n",
    "\n",
    "Hence, the matrix $A + R$ is diagonally strictly dominant. It means that the Gershorin circle of the matrix $A + R$ contains only positive numbers, implying that all the eigenvalues of $A + R$ are positive. Thus, we know that the matrix $A + R$ is definite positive. Moreover, it is also symmetric. Therefore, it possesses an Imcomplete Cholesky factorization, meaning that there exists a lower triangular matrix $L$ such that $A + R = L L^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Consider the following stationary iteration method: with $A = M-R = LL^T-R$ as above, define the sequence $V^k$ by choosing $V^0\\in \\mathbb R^N$ and setting $MV^{k+1} = b + RV^k$. \n",
    "\n",
    "a) Write down the resulting iterative method obtained by choosing the matrix of the iteration method $M = L L^T$ as the one of the incomplete Cholesky decomposition. What algorithms would you use to compute $V^{k+1}$?\n",
    "\n",
    "b) Implement this method and test it with the same parameter as in 3). \n",
    "\n",
    "*Hints:*\n",
    "- *in order to simplify the implementation, you may use the functions cho_factor and cho_solve from scipy.linalg to compute the incomplete Cholesky factorization and to compute $M^{-1}$. Reading the documentation (https://docs.scipy.org/doc/scipy/reference/linalg.html) of those functions is recommended before using them.*\n",
    "- *You may exploit the implementation of Jacobi algorithm (given above) to construct this one.*\n",
    "\n",
    "c) Compute its convergence rate numerically and compare it with the one of Jacobi method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "a) By choosing $M = L L^T$, we obtain that $L L^T V^{k+1} = b + R V^{k}$. We will compute $V^{k+1}$ in the following way. \n",
    "\n",
    "First, we compute the value of $V_1 = L^T V^{k+1}$ by solving the algorithm \n",
    "\n",
    "$$ \n",
    "L V_1 = B \\qquad \\text{with} \\quad B = b + R V^{k} \n",
    "$$ \n",
    "\n",
    "Next, we compute the value of $V^{k+1}$ by solving the algorithm \n",
    "\n",
    "$$ \n",
    "L^T V^{k+1} = V_1 \\qquad \\Longrightarrow \\qquad V^{k+1} = \\left( L^T \\right)^{-1} V_1 \n",
    "$$ \n",
    "\n",
    "Hence, we could use this method to obtain the values of $V^{k+1}$. \n",
    "\n",
    "b) Please see the code below. \n",
    "\n",
    "c) We compute the spectral radii for both algorithm. Since the spectral radius of the Incomplete Cholesky algorithm is smaller than the spectral radius of the Jacobi algorithm, we can conclude that the Incomplete Cholesky algorithm converges faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Incomplete_Cholesky_iterative(A, b, V0, it_max=1000, TOL=10**(-10)):\n",
    "    \"\"\"\n",
    "    Compute the iterations of the incomplete LU preconditioned Jacobi algorithm\n",
    "    ----------   \n",
    "    parameters: \n",
    "    A      : Matrix of size NxN of the problem A V = b\n",
    "    b      : RHS of the problem A V = b\n",
    "    V0     : initial vector in the method\n",
    "    it_max : maximum number of iterations\n",
    "    TOL    : use || A V^k - b || < TOL as a stopping criteria \n",
    "    \n",
    "    returns: \n",
    "    V       : the solution at the end of the iterations\n",
    "    tab_err : array of the || A V^k - b ||^2 at every iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    # initializations \n",
    "    V_new      = np.copy(V0) \n",
    "    V          = np.copy(V_new) \n",
    "    tab_err    = np.zeros(it_max) \n",
    "    tab_err[0] = lin.norm(np.matmul(A, V_new) - b) \n",
    "    \n",
    "    # construction of the matrices \n",
    "    N = len(b) \n",
    "    \n",
    "    R         = np.zeros((N, N)) \n",
    "    R[0, N-1] = 1 \n",
    "    R[N-1, 0] = 1 \n",
    "    \n",
    "    L = cho_factor(A + R) \n",
    "    \n",
    "    # convergence loop\n",
    "    for k in range(it_max-1):\n",
    "        V = np.copy(V_new) \n",
    "        \n",
    "        if(tab_err[k] < TOL): break \n",
    "        \n",
    "        RV = np.matmul(R, V) \n",
    "        V_new = cho_solve(L, b+RV) \n",
    "        \n",
    "        tab_err[k+1] = lin.norm(np.matmul(A, V_new) - b) \n",
    "    \n",
    "    return V, tab_err[:k] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "N  = 100\n",
    "A  = Construct_Matrix(N)\n",
    "b  = np.ones(N)\n",
    "V0 = np.zeros(N)\n",
    "\n",
    "#solving the system\n",
    "V_PJacobi, err_PJacobi = Incomplete_Cholesky_iterative(A, b, V0)\n",
    "\n",
    "#plot the errors\n",
    "plt.figure(4)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(err_Jacobi)\n",
    "plt.plot(err_PJacobi)\n",
    "plt.legend([\"Jacobi\",\"Incomplete Cholesky iterative\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_IChoIt = err_PJacobi[-1] / err_PJacobi[-2] \n",
    "print(f\"rho Jacobi = {rho_Jacobi}\") \n",
    "print(f\"rho IChoIt = {rho_IChoIt}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
